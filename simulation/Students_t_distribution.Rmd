---
title: "STAT480-Simulation"
author: "Quan Zhao"
date: "2023-12-19"
bibliography: references.bib
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
set.seed(123)

library(ggplot2)
```

# Introduce

The Student's t-distribution was first introduced by William Sealy Gosset in 1908, under the pseudonym 'Student,' in a paper published with the identifier @student1908probable. Ronald Aylmer Fisher and others, referenced with @fisher1925043, discussed the application of the Student's distribution in 1925. Similarly, Mohammad Ahsanullah and colleagues, cited as @ahsanullah2014normal, explored its application in 2014.

The Student's t-distribution is a fundamental statistical probability distribution essential for estimating the means of small sample sizes when the population standard deviation is unknown. Developed by William Sealy Gosset under the pseudonym "Student," this distribution distinguishes itself with heavier tails than the normal distribution. This key feature allows it to accommodate the increased variability found in smaller samples, making it particularly effective for data sets with fewer observations.

Its applications extend across hypothesis testing and confidence interval estimation, where the t-distribution is crucial for performing t-tests to compare means accurately, despite limited data availability. This makes the t-distribution a cornerstone in fields such as psychology, economics, and any discipline requiring precise decision-making based on small data sets.

## PDF and CDF of dist

PDF of student's t-distribution

$$
f(t) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\,\Gamma\left(\frac{\nu}{2}\right)} \left(1 + \frac{t^2}{\nu} \right)^{-\frac{\nu + 1}{2}}
$$

CDF of student's t-distribution

$$
F(t) = \int_{-\infty}^{t} f(u)\, du
$$

## Implement Student's PDF and CDF funcitons

```{r}
# Student's t PDF
student_t_pdf <- function(t, nu) {
  gamma((nu + 1) / 2) / (sqrt(nu * pi) * gamma(nu / 2)) * (1 + (t^2) / nu)^(-(nu + 1) / 2)
}

# Student's t CDF (using numerical integration)
student_t_cdf <- function(t, nu) {
  integrate(function(x) student_t_pdf(x, nu), lower = -Inf, upper = t)$value
}

nu <- 5 # degrees of freedom
t_value <- 2

# Calculate PDF
pdf_value <- student_t_pdf(t_value, nu)

# Calculate CDF
cdf_value <- student_t_cdf(t_value, nu)

# Print results
print(paste("PDF at t =", t_value, ":", pdf_value))
print(paste("CDF at t =", t_value, ":", cdf_value))

```

# Plot examples of the density; display the parameters that my chose.

Set Degree of freedom as 10

```{r}
# Set degrees of freedom
nu <- 10

plot_dist <- function(nu) {

# Create a sequence of t values
t_values <- seq(-5, 5, by = 0.1)

# Calculate PDF and CDF for each t value
pdf_values <- sapply(t_values, student_t_pdf, nu = nu)
cdf_values <- sapply(t_values, student_t_cdf, nu = nu)

# Create a data frame for plotting
data_for_plot <- data.frame(t = t_values, PDF = pdf_values, CDF = cdf_values)

# Plot PDF
pdf_plot <- ggplot(data_for_plot, aes(x = t, y = PDF)) +
  geom_line(color = 'blue') +
  ggtitle('PDF of Student\'s t-Distribution') +
  xlab('t value') +
  ylab('Density')

# Plot CDF
cdf_plot <- ggplot(data_for_plot, aes(x = t, y = CDF)) +
  geom_line(color = 'red') +
  ggtitle('CDF of Student\'s t-Distribution') +
  xlab('t value') +
  ylab('Cumulative Probability')

# Display plots
print(pdf_plot)
print(cdf_plot)
}

plot_dist(nu)
```

When the Degree of freedom is 1, then Student's t-distribution is standard Cauchy distribution.

```{r}
# Set degrees of freedom
nu <- 1
plot_dist(nu)
```

# Propose your own pseudorandom number generator that produces deviates from the Student's t distribution

```{r}
generate_normal <- function(n) {
  # Generate n/2 pairs of uniform random numbers
  u1 <- runif(n / 2)
  u2 <- runif(n / 2)
  
  # Box-Muller transform
  z1 <- sqrt(-2 * log(u1)) * cos(2 * pi * u2)
  z2 <- sqrt(-2 * log(u1)) * sin(2 * pi * u2)
  
  # Combine the two sets of normal deviates
  z <- c(z1, z2)
  return(z)
}

generate_chi_squared <- function(n, df) {
  # Generate df sets of n standard normal deviates
  normals <- matrix(generate_normal(n * df), nrow = n, ncol = df)
  
  # Compute the sum of squares across columns for chi-squared deviates
  chi_sq <- rowSums(normals^2)
  return(chi_sq)
}

generate_t_dist <- function(n, df) {
  # Generate n standard normal deviates
  Z <- generate_normal(n)
  
  # Generate n chi-squared deviates with df degrees of freedom
  V <- generate_chi_squared(n, df)
  
  # Compute the t-distributed random numbers
  T <- Z / sqrt(V / df)
  
  return(T)
}

# Example usage
set.seed(123) # For reproducibility
t_samples <- generate_t_dist(1000, 10) # Generate 1000 samples
hist(t_samples, breaks=40, main="Histogram of samples from Student's t-distribution")

```

# Given $X = (X_{1},X_{2}, ...,X_{n})$ a random sample of i.i.d. Student's t variables

```{r}
# log likelihood from scratch
# data use my generated

log_likelihood_v <- function(v, data) {
  if(v <= 0) return(Inf) # Ensure v is positive
  n <- length(data)
  mu <- 0 # Assuming mean is 0
  sigma <- 1 # Assuming standard deviation is 1
  gamma_part <- lgamma((v + 1) / 2) - lgamma(v / 2) - 0.5*log(v*pi) - log(sigma)
  summand <- -(v + 1) / 2 * log(1 + (data - mu)^2 / (v * sigma^2))
  log_likelihood <- sum(gamma_part + summand)
  return(-log_likelihood) # Return negative for minimization
}

# Use optim to find the MLE of v
find_mle_v <- function(data) {
  result <- optim(par = 5, fn = log_likelihood_v, data = data, method = "L-BFGS-B", lower = 0.001, upper = 100)
  return(result$par)
}


mle_v <- find_mle_v(t_samples)
mle_v
```

# Improve by Bootstrap

```{r}

# bootstrap impove for
# log likelihood from scratch
# data use my generated

bootstrap_mle <- function(data, n_bootstrap = 1000) {
  # Initialize a vector to store bootstrap MLE estimates
  bootstrap_estimates <- numeric(n_bootstrap)
  
  # Perform bootstrap
  for (i in 1:n_bootstrap) {
    # Sample with replacement
    bootstrap_sample <- sample(data, size = length(data), replace = TRUE)
    # Estimate parameter(s) using MLE on the bootstrap sample
    bootstrap_estimates[i] <- find_mle_v(bootstrap_sample)
  }
  
  # Calculate the mean of the bootstrap estimates
  mean_estimate <- mean(bootstrap_estimates)
  # Calculate the standard error of the bootstrap estimates
  std_error <- sd(bootstrap_estimates)
  
  # Optionally calculate confidence intervals, e.g., 95% CI
  ci_lower <- quantile(bootstrap_estimates, probs = 0.025)
  ci_upper <- quantile(bootstrap_estimates, probs = 0.975)
  
  # Return a list with the bootstrap results
  return(list(mean_estimate = mean_estimate, std_error = std_error, ci_lower = ci_lower, ci_upper = ci_upper))
}

bootstrap_results <- bootstrap_mle(t_samples)
print(bootstrap_results)

```

# Study the Bias and MSE

```{r}
library(parallel) # For parallel computation

# Function to simulate study for a single scenario
simulate_study <- function(n, v_true, n_bootstrap = 1000) {
  # Generate data from Student's t-distribution
  data <- rt(n = n, df = v_true)
  
  # Apply MLE to estimate v
  mle_estimate <- find_mle_v(data)
  
  # Apply Bootstrap to estimate v
  bootstrap_result <- bootstrap_mle(data, n_bootstrap)
  bootstrap_estimate <- bootstrap_result$mean_estimate
  
  # Calculate bias and MSE for MLE and Bootstrap
  bias_mle <- mle_estimate - v_true
  mse_mle <- bias_mle^2
  
  bias_bootstrap <- bootstrap_estimate - v_true
  mse_bootstrap <- bias_bootstrap^2
  
  # Return results
  list(n = n, v_true = v_true, bias_mle = bias_mle, mse_mle = mse_mle, 
       bias_bootstrap = bias_bootstrap, mse_bootstrap = mse_bootstrap)
}

# Scenarios to study: combinations of sample sizes and degrees of freedom
sample_sizes <- c(30, 50, 300, 1000, 2000)
df_values <- c(10)

# Perform study across all scenarios
results <- expand.grid(n = sample_sizes, v_true = df_values)
study_results <- mclapply(1:nrow(results), function(i) {
  simulate_study(results$n[i], results$v_true[i])
}, mc.cores = detectCores() - 1)

# Convert results to a dataframe for easier analysis
study_results_df <- do.call(rbind, lapply(study_results, function(x) data.frame(x)))

# Analyze the results
# print(study_results_df)

# plot

# Convert 'study_results_df' to a long format for easier plotting with ggplot2
study_results_long <- tidyr::pivot_longer(study_results_df, 
                                          cols = c(mse_mle, mse_bootstrap, bias_mle, bias_bootstrap),
                                          names_to = "Estimation_Method", 
                                          values_to = "Value")

# Splitting the data frame into two: one for MSE and one for Bias for clarity in plotting
mse_data <- subset(study_results_long, Estimation_Method %in% c("mse_mle", "mse_bootstrap"))
bias_data <- subset(study_results_long, Estimation_Method %in% c("bias_mle", "bias_bootstrap"))

# Adjusting the Estimation_Method factor for proper labeling
mse_data$Estimation_Method <- factor(mse_data$Estimation_Method, labels = c("MLE", "Bootstrap"))
bias_data$Estimation_Method <- factor(bias_data$Estimation_Method, labels = c("MLE", "Bootstrap"))

# Plot for MSE
ggplot(mse_data, aes(x = n, y = Value, colour = Estimation_Method)) +
  geom_line() +
  labs(title = "MSE of MLE and Bootstrap Estimates",
       x = "Sample Size",
       y = "MSE",
       colour = "Estimation Method") +
  theme_minimal()

# Plot for Bias
ggplot(bias_data, aes(x = n, y = Value, colour = Estimation_Method)) +
  geom_line() +
  labs(title = "Bias of MLE and Bootstrap Estimates",
       x = "Sample Size",
       y = "Bias",
       colour = "Estimation Method") +
  theme_minimal()

```

Fix sample size as 300, select Parameter V

```{r}
# Function to simulate study for a single scenario
simulate_study <- function(n, v_true, n_bootstrap = 1000) {
  # Generate data from Student's t-distribution
  data <- rt(n = n, df = v_true)
  
  # Apply MLE to estimate v
  mle_estimate <- find_mle_v(data)
  
  # Apply Bootstrap to estimate v
  bootstrap_result <- bootstrap_mle(data, n_bootstrap)
  bootstrap_estimate <- bootstrap_result$mean_estimate
  
  # Calculate bias and MSE for MLE and Bootstrap
  bias_mle <- mle_estimate - v_true
  mse_mle <- bias_mle^2
  
  bias_bootstrap <- bootstrap_estimate - v_true
  mse_bootstrap <- bias_bootstrap^2
  
  # Return results
  list(n = n, v_true = v_true, bias_mle = bias_mle, mse_mle = mse_mle, 
       bias_bootstrap = bias_bootstrap, mse_bootstrap = mse_bootstrap)
}

# Scenarios to study: combinations of sample sizes and degrees of freedom
sample_sizes <- c(300)
df_values <- c(1, 3, 5, 7, 10, 50, 100)

# Perform study across all scenarios
results <- expand.grid(n = sample_sizes, v_true = df_values)
study_results <- mclapply(1:nrow(results), function(i) {
  simulate_study(results$n[i], results$v_true[i])
}, mc.cores = detectCores() - 1)

# Convert results to a dataframe for easier analysis
study_results_fix_sample_size_df <- do.call(rbind, lapply(study_results, function(x) data.frame(x)))

# Analyze the results
print(study_results_fix_sample_size_df)

# plot

# Convert 'study_results_df' to a long format for easier plotting with ggplot2
study_results_long <- tidyr::pivot_longer(study_results_fix_sample_size_df, 
                                          cols = c(mse_mle, mse_bootstrap, bias_mle, bias_bootstrap),
                                          names_to = "Estimation_Method", 
                                          values_to = "Value")

# Splitting the data frame into two: one for MSE and one for Bias for clarity in plotting
mse_data <- subset(study_results_long, Estimation_Method %in% c("mse_mle", "mse_bootstrap"))
bias_data <- subset(study_results_long, Estimation_Method %in% c("bias_mle", "bias_bootstrap"))

# Adjusting the Estimation_Method factor for proper labeling
mse_data$Estimation_Method <- factor(mse_data$Estimation_Method, labels = c("MLE", "Bootstrap"))
bias_data$Estimation_Method <- factor(bias_data$Estimation_Method, labels = c("MLE", "Bootstrap"))

# Plot for MSE
ggplot(mse_data, aes(x = v_true, y = Value, colour = Estimation_Method)) +
  geom_line() +
  labs(title = "MSE of MLE and Bootstrap Estimates",
       x = "Sample Size",
       y = "MSE",
       colour = "Estimation Method") +
  theme_minimal()

# Plot for Bias
ggplot(bias_data, aes(x = v_true, y = Value, colour = Estimation_Method)) +
  geom_line() +
  labs(title = "Bias of MLE and Bootstrap Estimates",
       x = "Sample Size",
       y = "Bias",
       colour = "Estimation Method") +
  theme_minimal()
```

# Experiments

In the experiments, we first fix the Degree of Freedom (DF) at 10 to select the best sample size. Based on the plot of Mean Squared Error (MSE) and Bias, a sample size of 300 gives us the elbow point.

Then, we fix the sample size at 300 to select the best DF. From the MSE and Bias plot, we found that a DF around 10 indicates the elbow point.

# Analysis

# Comparing MSE and Bias for MLE and Bootstrap

Based on the plots, we found that Bootstrap performs very well when the sample size is small. It shows similar performance when the sample size becomes larger. However, because it costs more than the Maximum Likelihood Estimation (MLE) method, there is no advantage to using it when we have sufficient data samples.

# Conclusion

We initiated our study by generating Student's t-distributions from scratch, which served as the foundation for our Maximum Likelihood Estimation (MLE) analysis. Subsequently, we employed bootstrap methods to refine our MLE estimates. Our findings underscore the significance of selecting suitable Degrees of Freedom (DF) and sample sizes. Specifically, a sample size of 300 with the DF set at 10 was identified as providing an optimal equilibrium between Mean Squared Error (MSE) and Bias. Our comparative analysis of MLE and Bootstrap approaches further indicated that while Bootstrap holds benefits for smaller sample sizes due to performance comparable to MLE, its greater computational demand renders it less favorable for larger datasets. Therefore, MLE stands out as the more viable option for substantial data collections, highlighting the necessity of choosing the appropriate statistical method based on the size of the dataset and computational resources.


# Reference
