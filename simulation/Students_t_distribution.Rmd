---
title: "STAT480-Simulation"
author: "Quan Zhao"
date: "2023-12-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
set.seed(123)

library(ggplot2)
```

# Introduce

The Student's t-distribution is a fundamental statistical probability distribution, essential for estimating the means of small sample sizes when the population standard deviation is unknown. Developed by William Sealy Gosset under the pseudonym "Student," this distribution distinguishes itself with heavier tails than the normal distribution. This key feature allows it to accommodate the increased variability found in smaller samples, making it particularly effective for data sets with fewer observations.

Its applications extend across hypothesis testing and confidence interval estimation, where the t-distribution is crucial for performing t-tests to compare means accurately, despite limited data availability. This makes the t-distribution a cornerstone in fields such as psychology, economics, and any discipline requiring precise decision-making based on small data sets.

## PDF and CDF of dist

PDF of student's t-distribution

$$
f(t) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\,\Gamma\left(\frac{\nu}{2}\right)} \left(1 + \frac{t^2}{\nu} \right)^{-\frac{\nu + 1}{2}}
$$

CDF of student's t-distribution

$$
F(t) = \int_{-\infty}^{t} f(u)\, du
$$

## Implement Student's PDF and CDF funcitons

```{r}
# Student's t PDF
student_t_pdf <- function(t, nu) {
  gamma((nu + 1) / 2) / (sqrt(nu * pi) * gamma(nu / 2)) * (1 + (t^2) / nu)^(-(nu + 1) / 2)
}

# Student's t CDF (using numerical integration)
student_t_cdf <- function(t, nu) {
  integrate(function(x) student_t_pdf(x, nu), lower = -Inf, upper = t)$value
}

# Example usage
nu <- 5 # degrees of freedom
t_value <- 2

# Calculate PDF
pdf_value <- student_t_pdf(t_value, nu)

# Calculate CDF
cdf_value <- student_t_cdf(t_value, nu)

# Print results
print(paste("PDF at t =", t_value, ":", pdf_value))
print(paste("CDF at t =", t_value, ":", cdf_value))

```

# Plot examples of the density; display the parameters that my chose.

Set Degree of freedom as 10

```{r}
# Load necessary library
library(ggplot2)

# Set degrees of freedom
nu <- 10

plot_dist <- function(nu) {

# Create a sequence of t values
t_values <- seq(-5, 5, by = 0.1)

# Calculate PDF and CDF for each t value
pdf_values <- sapply(t_values, student_t_pdf, nu = nu)
cdf_values <- sapply(t_values, student_t_cdf, nu = nu)

# Create a data frame for plotting
data_for_plot <- data.frame(t = t_values, PDF = pdf_values, CDF = cdf_values)

# Plot PDF
pdf_plot <- ggplot(data_for_plot, aes(x = t, y = PDF)) +
  geom_line(color = 'blue') +
  ggtitle('PDF of Student\'s t-Distribution') +
  xlab('t value') +
  ylab('Density')

# Plot CDF
cdf_plot <- ggplot(data_for_plot, aes(x = t, y = CDF)) +
  geom_line(color = 'red') +
  ggtitle('CDF of Student\'s t-Distribution') +
  xlab('t value') +
  ylab('Cumulative Probability')

# Display plots
print(pdf_plot)
print(cdf_plot)
}

plot_dist(nu)
```

When the Degree of freedom is 1, then Student's t-distribution is standard Cauchy distribution.

```{r}
nu <- 1
plot_dist(nu)
```

# Propose your own pseudorandom number generator that produces deviates from the Student's t distribution

```{r}
# this not use runif only, thus ignore it

# generate_t_distribution <- function(n, nu) {
#   Z <- rnorm(n)
#   X <- rchisq(n, df = nu)
#   T <- Z / sqrt(X / nu)
#   return(T)
# }
# 
# # Example usage
# n <- 1000    # number of random values to generate
# nu <- 5      # degrees of freedom
# t_values <- generate_t_distribution(n, nu)
# 
# # You can plot to see the distribution
# hist(t_values, breaks = 30, main = "Generated Student's t-Distribution", xlab = "T-values")

```

```{r}
generate_normal <- function(n) {
  # Generate n/2 pairs of uniform random numbers
  u1 <- runif(n / 2)
  u2 <- runif(n / 2)
  
  # Box-Muller transform
  z1 <- sqrt(-2 * log(u1)) * cos(2 * pi * u2)
  z2 <- sqrt(-2 * log(u1)) * sin(2 * pi * u2)
  
  # Combine the two sets of normal deviates
  z <- c(z1, z2)
  return(z)
}

generate_chi_squared <- function(n, df) {
  # Generate df sets of n standard normal deviates
  normals <- matrix(generate_normal(n * df), nrow = n, ncol = df)
  
  # Compute the sum of squares across columns for chi-squared deviates
  chi_sq <- rowSums(normals^2)
  return(chi_sq)
}

generate_t_dist <- function(n, df) {
  # Generate n standard normal deviates
  Z <- generate_normal(n)
  
  # Generate n chi-squared deviates with df degrees of freedom
  V <- generate_chi_squared(n, df)
  
  # Compute the t-distributed random numbers
  T <- Z / sqrt(V / df)
  
  return(T)
}

# Example usage
set.seed(123) # For reproducibility
t_samples <- generate_t_dist(1000, 10) # Generate 1000 samples
hist(t_samples, breaks=40, main="Histogram of samples from Student's t-distribution")

```

# 5. Given $X = (X_{1},X_{2}, ...,X_{n})$ a random sample of i.i.d. Student's t variables,

## obtain your own ***maximum likelihood estimator*** for (a,b), say $\widehat{(a,b)}_{ML}$. Implement it.

```{r}
# # Student's t PDF function
# student_t_pdf <- function(t, nu) {
#   gamma((nu + 1) / 2) / (sqrt(nu * pi) * gamma(nu / 2)) * (1 + (t^2) / nu)^(-(nu + 1) / 2)
# }
# 
# # Log-Likelihood function using the custom Student's t PDF
# log_likelihood_t <- function(v, data) {
#   sum(log(sapply(data, student_t_pdf, nu = v)))
# }
# 
# # Function to find MLE of degrees of freedom (v)
# mle_t_distribution <- function(data) {
#   # Ensure data does not contain non-finite values
#   data <- data[is.finite(data)]
#   
#   # Optim function with bounds for v and adjusted initial value
#   mle <- optim(par = 4, fn = function(v) -log_likelihood_t(v, data), 
#                method = "L-BFGS-B", lower = 2, upper = 30, control = list(fnscale = -1))
#   return(mle$par)
# }
# 
# # Generate random data using the custom Student's t distribution function
# data_sample <- generate_t_distribution(100, 5)  # Example data generation
# 
# # Estimate degrees of freedom using MLE
# estimated_v <- mle_t_distribution(data_sample)
# print(estimated_v)
```

```{r}
# # Log-Likelihood function for the Student's t-distribution
# log_likelihood <- function(v, data) {
#   n <- length(data)
#   log_likelihood <- sum(stats::dt(data, df=v, log=TRUE))
#   return(-log_likelihood)  # Return negative because 'optim' minimizes
# }
# 
# # MLE Estimator for degrees of freedom of Student's t-distribution
# mle_student_t <- function(data) {
#   # Initial guess for degrees of freedom
#   start_v <- 10
#   
#   # Optimize the log-likelihood function
#   mle <- optim(par = start_v, fn = log_likelihood, data = data, method = "L-BFGS-B", lower = 2.01, upper = 100)
#   
#   # Return the estimated degrees of freedom
#   return(mle$par)
# }
# 
# # Example usage with simulated data
# set.seed(123)
# # Generate some data from a Student's t-distribution with 5 degrees of freedom
# data <- rt(100, df = 5)
# 
# # Estimate the degrees of freedom
# estimated_df <- mle_student_t(data)
# estimated_df

```

```{r}

# # log likelihood from scartch
# 
# log_likelihood_v <- function(v, data) {
#   if(v <= 0) return(Inf) # Ensure v is positive
#   n <- length(data)
#   mu <- 0 # Assuming mean is 0
#   sigma <- 1 # Assuming standard deviation is 1
#   gamma_part <- lgamma((v + 1) / 2) - lgamma(v / 2) - 0.5*log(v*pi) - log(sigma)
#   summand <- -(v + 1) / 2 * log(1 + (data - mu)^2 / (v * sigma^2))
#   log_likelihood <- sum(gamma_part + summand)
#   return(-log_likelihood) # Return negative for minimization
# }
# 
# # Use optim to find the MLE of v
# find_mle_v <- function(data) {
#   result <- optim(par = 5, fn = log_likelihood_v, data = data, method = "L-BFGS-B", lower = 0.001, upper = 100)
#   return(result$par)
# }
# 
# # Example usage
# set.seed(123)
# data <- rt(n = 100, df = 5) # Simulated data for testing
# mle_v <- find_mle_v(data)
# mle_v

```

```{r}
# log likelihood from scartch
# sample use my generated

log_likelihood_v <- function(v, data) {
  if(v <= 0) return(Inf) # Ensure v is positive
  n <- length(data)
  mu <- 0 # Assuming mean is 0
  sigma <- 1 # Assuming standard deviation is 1
  gamma_part <- lgamma((v + 1) / 2) - lgamma(v / 2) - 0.5*log(v*pi) - log(sigma)
  summand <- -(v + 1) / 2 * log(1 + (data - mu)^2 / (v * sigma^2))
  log_likelihood <- sum(gamma_part + summand)
  return(-log_likelihood) # Return negative for minimization
}

# Use optim to find the MLE of v
find_mle_v <- function(data) {
  result <- optim(par = 5, fn = log_likelihood_v, data = data, method = "L-BFGS-B", lower = 0.001, upper = 100)
  return(result$par)
}

# Example usage
set.seed(123)
# data <- rt(n = 100, df = 5) # Simulated data for testing
mle_v <- find_mle_v(t_samples)
mle_v
```

# Improve by Bootstrap

```{r}
# # Bootstrap-enhanced MLE function
# bootstrap_mle_t_distribution <- function(original_data, n_bootstrap = 1000) {
#   bootstrap_estimates <- numeric(n_bootstrap)
# 
#   for (i in 1:n_bootstrap) {
#     # Create a bootstrap sample
#     bootstrap_sample <- sample(original_data, size = length(original_data), replace = TRUE)
# 
#     # Estimate v using MLE for the bootstrap sample
#     estimated_v <- optim(par = 4, fn = function(v) -log_likelihood_t(v, bootstrap_sample), 
#                          method = "L-BFGS-B", lower = 2, upper = 30, control = list(fnscale = -1))$par
#     bootstrap_estimates[i] <- estimated_v
#   }
# 
#   # Aggregate results: mean or median can be used
#   final_estimate <- mean(bootstrap_estimates)
#   return(list(estimate = final_estimate, bootstrap_estimates = bootstrap_estimates))
# }
# 
# # Example usage
# data_sample <- generate_t_distribution(100, 5)  # Generate sample data
# result <- bootstrap_mle_t_distribution(data_sample)
# print(result)
```

```{r}

# bootstrap impove for
# log likelihood from scartch
# sample use my generated

bootstrap_mle <- function(data, n_bootstrap = 1000) {
  # Initialize a vector to store bootstrap MLE estimates
  bootstrap_estimates <- numeric(n_bootstrap)
  
  # Perform bootstrap
  for (i in 1:n_bootstrap) {
    # Sample with replacement
    bootstrap_sample <- sample(data, size = length(data), replace = TRUE)
    # Estimate parameter(s) using MLE on the bootstrap sample
    bootstrap_estimates[i] <- find_mle_v(bootstrap_sample)
  }
  
  # Calculate the mean of the bootstrap estimates
  mean_estimate <- mean(bootstrap_estimates)
  # Calculate the standard error of the bootstrap estimates
  std_error <- sd(bootstrap_estimates)
  
  # Optionally calculate confidence intervals, e.g., 95% CI
  ci_lower <- quantile(bootstrap_estimates, probs = 0.025)
  ci_upper <- quantile(bootstrap_estimates, probs = 0.975)
  
  # Return a list with the bootstrap results
  return(list(mean_estimate = mean_estimate, std_error = std_error, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Example usage
set.seed(123)
data <- rt(n = 100, df = 5) # Simulated data for testing
bootstrap_results <- bootstrap_mle(data)
print(bootstrap_results)

```

# Study the Bias and MSE

```{r}
# # Assuming find_mle_v(data) and bootstrap_mle(data, n_bootstrap) are defined as before
# 
# # Function to simulate and calculate bias and MSE
# simulate_study <- function(v_true, sample_sizes, n_simulations = 100, n_bootstrap = 500) {
#   results <- expand.grid(sample_size = sample_sizes, sim = 1:n_simulations)
#   results$mle_v <- NA
#   results$bootstrap_mean_v <- NA
#   
#   for (i in 1:nrow(results)) {
#     # Generate data
#     data <- rt(n = results$sample_size[i], df = v_true)
#     
#     # MLE estimation
#     mle_v <- find_mle_v(data)
#     results$mle_v[i] <- mle_v
#     
#     # Bootstrap estimation
#     bootstrap_res <- bootstrap_mle(data, n_bootstrap)
#     results$bootstrap_mean_v[i] <- bootstrap_res$mean_estimate
#   }
#   
#   # Calculate bias and MSE for MLE and Bootstrap
#   results$mle_bias <- results$mle_v - v_true
#   results$bootstrap_bias <- results$bootstrap_mean_v - v_true
#   results$mle_mse <- results$mle_bias^2
#   results$bootstrap_mse <- results$bootstrap_bias^2
#   
#   # Aggregate results
#   summary <- aggregate(cbind(mle_bias, bootstrap_bias, mle_mse, bootstrap_mse) ~ sample_size, data = results, mean)
#   
#   return(summary)
# }
# 
# # Example usage
# sample_sizes <- c(30, 50, 100, 200, 300, 500)
# v_true <- 5
# study_results <- simulate_study(v_true, sample_sizes)
# print(study_results)

```

```{r}
# library(ggplot2)
# 
# # Assuming study_results is the output from the simulate_study function
# 
# # Plot for MSE
# ggplot(study_results, aes(x = sample_size)) +
#   geom_line(aes(y = mle_mse, colour = "MLE")) +
#   geom_line(aes(y = bootstrap_mse, colour = "Bootstrap")) +
#   labs(title = "MSE of MLE and Bootstrap Estimates",
#        x = "Sample Size",
#        y = "MSE",
#        colour = "Estimation Method") +
#   theme_minimal()
# 
# # Plot for Bias
# ggplot(study_results, aes(x = sample_size)) +
#   geom_line(aes(y = mle_bias, colour = "MLE")) +
#   geom_line(aes(y = bootstrap_bias, colour = "Bootstrap")) +
#   labs(title = "Bias of MLE and Bootstrap Estimates",
#        x = "Sample Size",
#        y = "Bias",
#        colour = "Estimation Method") +
#   theme_minimal()

```

```{r}
library(parallel) # For parallel computation

# Assuming `find_mle_v(data)` is your function for estimating degrees of freedom using MLE
# Assuming `bootstrap_mle(data, n_bootstrap = 1000)` is your function for bootstrap estimation

# Function to simulate study for a single scenario
simulate_study <- function(n, v_true, n_bootstrap = 1000) {
  # Generate data from Student's t-distribution
  data <- rt(n = n, df = v_true)
  
  # Apply MLE to estimate v
  mle_estimate <- find_mle_v(data)
  
  # Apply Bootstrap to estimate v
  bootstrap_result <- bootstrap_mle(data, n_bootstrap)
  bootstrap_estimate <- bootstrap_result$mean_estimate
  
  # Calculate bias and MSE for MLE and Bootstrap
  bias_mle <- mle_estimate - v_true
  mse_mle <- bias_mle^2
  
  bias_bootstrap <- bootstrap_estimate - v_true
  mse_bootstrap <- bias_bootstrap^2
  
  # Return results
  list(n = n, v_true = v_true, bias_mle = bias_mle, mse_mle = mse_mle, 
       bias_bootstrap = bias_bootstrap, mse_bootstrap = mse_bootstrap)
}

# Scenarios to study: combinations of sample sizes and degrees of freedom
sample_sizes <- c(30, 50, 300, 1000, 2000)
df_values <- c(10)

# Perform study across all scenarios
results <- expand.grid(n = sample_sizes, v_true = df_values)
study_results <- mclapply(1:nrow(results), function(i) {
  simulate_study(results$n[i], results$v_true[i])
}, mc.cores = detectCores() - 1)

# Convert results to a dataframe for easier analysis
study_results_df <- do.call(rbind, lapply(study_results, function(x) data.frame(x)))

# Analyze the results
# print(study_results_df)

# plot

# Convert 'study_results_df' to a long format for easier plotting with ggplot2
study_results_long <- tidyr::pivot_longer(study_results_df, 
                                          cols = c(mse_mle, mse_bootstrap, bias_mle, bias_bootstrap),
                                          names_to = "Estimation_Method", 
                                          values_to = "Value")

# Splitting the data frame into two: one for MSE and one for Bias for clarity in plotting
mse_data <- subset(study_results_long, Estimation_Method %in% c("mse_mle", "mse_bootstrap"))
bias_data <- subset(study_results_long, Estimation_Method %in% c("bias_mle", "bias_bootstrap"))

# Adjusting the Estimation_Method factor for proper labeling
mse_data$Estimation_Method <- factor(mse_data$Estimation_Method, labels = c("MLE", "Bootstrap"))
bias_data$Estimation_Method <- factor(bias_data$Estimation_Method, labels = c("MLE", "Bootstrap"))

# Plot for MSE
ggplot(mse_data, aes(x = n, y = Value, colour = Estimation_Method)) +
  geom_line() +
  labs(title = "MSE of MLE and Bootstrap Estimates",
       x = "Sample Size",
       y = "MSE",
       colour = "Estimation Method") +
  theme_minimal()

# Plot for Bias
ggplot(bias_data, aes(x = n, y = Value, colour = Estimation_Method)) +
  geom_line() +
  labs(title = "Bias of MLE and Bootstrap Estimates",
       x = "Sample Size",
       y = "Bias",
       colour = "Estimation Method") +
  theme_minimal()

```

Fix sample size as 300, select Parameter V

```{r}
library(parallel) # For parallel computation

# Assuming `find_mle_v(data)` is your function for estimating degrees of freedom using MLE
# Assuming `bootstrap_mle(data, n_bootstrap = 1000)` is your function for bootstrap estimation

# Function to simulate study for a single scenario
simulate_study <- function(n, v_true, n_bootstrap = 1000) {
  # Generate data from Student's t-distribution
  data <- rt(n = n, df = v_true)
  
  # Apply MLE to estimate v
  mle_estimate <- find_mle_v(data)
  
  # Apply Bootstrap to estimate v
  bootstrap_result <- bootstrap_mle(data, n_bootstrap)
  bootstrap_estimate <- bootstrap_result$mean_estimate
  
  # Calculate bias and MSE for MLE and Bootstrap
  bias_mle <- mle_estimate - v_true
  mse_mle <- bias_mle^2
  
  bias_bootstrap <- bootstrap_estimate - v_true
  mse_bootstrap <- bias_bootstrap^2
  
  # Return results
  list(n = n, v_true = v_true, bias_mle = bias_mle, mse_mle = mse_mle, 
       bias_bootstrap = bias_bootstrap, mse_bootstrap = mse_bootstrap)
}

# Scenarios to study: combinations of sample sizes and degrees of freedom
sample_sizes <- c(300)
df_values <- c(1, 3, 5, 7, 10, 50, 100)

# Perform study across all scenarios
results <- expand.grid(n = sample_sizes, v_true = df_values)
study_results <- mclapply(1:nrow(results), function(i) {
  simulate_study(results$n[i], results$v_true[i])
}, mc.cores = detectCores() - 1)

# Convert results to a dataframe for easier analysis
study_results_fix_sample_size_df <- do.call(rbind, lapply(study_results, function(x) data.frame(x)))

# Analyze the results
print(study_results_fix_sample_size_df)

# plot

# Convert 'study_results_df' to a long format for easier plotting with ggplot2
study_results_long <- tidyr::pivot_longer(study_results_fix_sample_size_df, 
                                          cols = c(mse_mle, mse_bootstrap, bias_mle, bias_bootstrap),
                                          names_to = "Estimation_Method", 
                                          values_to = "Value")

# Splitting the data frame into two: one for MSE and one for Bias for clarity in plotting
mse_data <- subset(study_results_long, Estimation_Method %in% c("mse_mle", "mse_bootstrap"))
bias_data <- subset(study_results_long, Estimation_Method %in% c("bias_mle", "bias_bootstrap"))

# Adjusting the Estimation_Method factor for proper labeling
mse_data$Estimation_Method <- factor(mse_data$Estimation_Method, labels = c("MLE", "Bootstrap"))
bias_data$Estimation_Method <- factor(bias_data$Estimation_Method, labels = c("MLE", "Bootstrap"))

# Plot for MSE
ggplot(mse_data, aes(x = v_true, y = Value, colour = Estimation_Method)) +
  geom_line() +
  labs(title = "MSE of MLE and Bootstrap Estimates",
       x = "Sample Size",
       y = "MSE",
       colour = "Estimation Method") +
  theme_minimal()

# Plot for Bias
ggplot(bias_data, aes(x = v_true, y = Value, colour = Estimation_Method)) +
  geom_line() +
  labs(title = "Bias of MLE and Bootstrap Estimates",
       x = "Sample Size",
       y = "Bias",
       colour = "Estimation Method") +
  theme_minimal()
```

# Experiments

In the experiments, we first fix the Degree of Freedom (DF) at 10 to select the best sample size. Based on the plot of Mean Squared Error (MSE) and Bias, a sample size of 300 gives us the elbow point.

Then, we fix the sample size at 300 to select the best DF. From the MSE and Bias plot, we found that a DF around 10 indicates the elbow point.

# Analysis

# Comparing MSE and Bias for MLE and Bootstrap

Based on the plots, we found that Bootstrap performs very well when the sample size is small. It shows similar performance when the sample size becomes larger. However, because it costs more than the Maximum Likelihood Estimation (MLE) method, there is no advantage to using it when we have sufficient data samples.

# Conclusion

We initiated our study by generating Student's t-distributions from scratch, which served as the foundation for our Maximum Likelihood Estimation (MLE) analysis. Subsequently, we employed bootstrap methods to refine our MLE estimates. Our findings underscore the significance of selecting suitable Degrees of Freedom (DF) and sample sizes. Specifically, a sample size of 300 with the DF set at 10 was identified as providing an optimal equilibrium between Mean Squared Error (MSE) and Bias. Our comparative analysis of MLE and Bootstrap approaches further indicated that while Bootstrap holds benefits for smaller sample sizes due to performance comparable to MLE, its greater computational demand renders it less favorable for larger datasets. Therefore, MLE stands out as the more viable option for substantial data collections, highlighting the necessity of choosing the appropriate statistical method based on the size of the dataset and computational resources.
