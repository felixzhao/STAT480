\documentclass{article}

\usepackage{graphicx, amsmath}

% 0. Referencing style, APA-like referencing
\usepackage[backend=biber, style=apa, natbib=true]{biblatex}
\addbibresource{bibliography.bib} 

\usepackage[margin=1in]{geometry} % Setting the margins to 1 inch

\usepackage{amsmath}

\begin{document}

\title{
  STAT480 Research Preparation for Data Science\\
  Ordinal Data clustering and prediction}

\author{Quan Zhao\\
Email: felixz2010@gmail.com\\
Student ID 300471028}

\maketitle

\section{Introduction}

In the intricate field of data analysis, the study of ordinal data, which consists of categories with an inherent order but undefined spacing, presents unique challenges and opportunities. This paper delves into the complexities of clustering and predicting ordinal data, a prevalent type of data in fields ranging from social sciences to healthcare. Ordinal data distinguishes itself from nominal and continuous data types by necessitating specialized analytical methods that acknowledge its structured ordering without presuming equal intervals between its categories.

The initial part of our investigation defines ordinal data and underscores its distinctive features. This foundation supports a subsequent discussion on suitable clustering and prediction methodologies, with a focus on model-based clustering as a sophisticated approach tailored to the ordinal nature of the data. This contrasts with the limitations inherent in distance-based methods when applied to ordinal data. Additionally, the paper reviews specific statistical models developed for ordinal data analysis, such as the Proportional Odds model and the Ordered Stereotype Regression Model, highlighting their effectiveness in extracting meaningful insights.

Reflecting on the historical development of ordinal data analysis, the paper traces its evolution from early methodologies to recent innovations that have broadened its applicability and improved its precision. This historical context sets the stage for future research directions, emphasizing the potential for integrating machine learning with conventional statistical methods to refine the predictive performance of ordinal data models.

The author's endeavor to investigate the extension of these methodologies to the predictive clustering of new data. 
This exploration aims to bridge traditional statistical analysis with cutting-edge machine learning techniques, enhancing the ability to accurately predict cluster membership for incoming data points. 
Such advancements are crucial for applying ordinal data models in dynamic environments where predictive accuracy is paramount.

In summary, this paper offers a comprehensive examination of methodologies for clustering and predicting ordinal data, enriched by a forward-looking investigation into their extension for predictive clustering of new data. This inquiry is intended as a resource for both researchers and practitioners, providing insights necessary for navigating the complexities of ordinal data and leveraging its full potential in analytical endeavors.

\subsection{Ordinal Data}

Ordinal data, a pivotal concept in statistical analysis, represents categorical data characterized by a meaningful order among its categories, without implying uniform differences between these ranks. Unlike nominal data, which merely categorizes without any implied ranking, ordinal data elevates the categorical analysis by introducing a hierarchy or sequence that is significant yet lacks quantifiable intervals between its elements. This characteristic distinguishes ordinal data: it conveys the sequence of values but remains silent on the magnitude of difference between successive ranks.

Such data often appear as rankings or ordered classifications where the precise distance between categories is undefined or irrelevant. 
Despite sometimes being numerically coded for analytical convenience, ordinal data resist traditional arithmetic operations, rendering calculations like addition or subtraction inappropriate and misleading.

In the realm of statistical analysis, ordinal data necessitates specialized, non-parametric methods. 
The median and mode stand out as appropriate measures of central tendency for this data types, 
aligning with their ordered nature without assuming equal intervals between categories.

Ordinal data's prevalence is notably high in social sciences, especially in surveys and questionnaires designed to capture subjective assessments such as attitudes, opinions, and preferences. These instruments frequently employ scales—ranging from ``strongly disagree'' to ``strongly agree''—to elicit responses that, while ordered, do not support the notion of fixed distances between adjacent categories. This principle underlines the core attribute of ordinal data: the clear hierarchy among responses without an inherent interval scale to quantify the gaps between them. Consequently, assigning numerical values to such categories for computational purposes is generally discouraged and conceptually flawed, as it misinterprets the data's ordinal nature 

(~\textcite{Johnson1999}) gives a good sample,
 It is typically rational to presume an order such as

 \[
 \text{strongly disagree} < \text{disagree} < \text{don’t know} < \text{agree} < \text{strongly agree}
\]
 However, it is often illogical to allocate integer values to these categories. Therefore, calculations like
\[
\text{``disagree''} - \text{``strongly disagree''}
\]

\[
\text{``agree''} - \text{``don't know''}
\]
are not considered valid. 
This demonstrates that performing arithmetic calculations on the numerical values assigned to the categories does not make sense.

\subsubsection*{Comparison with continuous numerical data}

In the context of statistical analysis, the distinction between ordinal and continuous numerical data types is pivotal, influencing both the choice of analytical methods and the depth of insights that can be derived. 
Ordinal data, characterized by its capacity to rank order categories without indicating precise differences between them, is inherently less informative than continuous numerical data. 
Continuous numerical data, with its quantitative nature, allows for an infinite range of values and supports detailed statistical operations, including arithmetic calculations and the application of advanced statistical models, facilitating a nuanced understanding of variables and their interrelations (~\cite{Stevens1946}).

The limitations of ordinal data stem from its inability to quantify the exact magnitude of differences between categories, a factor that restricts the application of parametric statistical methods. This constraint necessitates reliance on non-parametric methods, focusing on medians and modes rather than means and standard deviations, thereby offering a less detailed analysis (~\cite{Conover1999}). 
For instance, in Likert scale responses commonly used in surveys, the ordinal nature of data precludes meaningful calculations of averages or differences between responses, limiting the depth of analysis that can be achieved (~\cite{Likert1932}).

Comparatively, continuous numerical data's capacity for precise measurement and the application of a broader range of statistical analyses enables a more detailed and accurate exploration of phenomena. 
This capability is contingent upon the availability of continuous numerical results, which may not always be feasible. The nature of the data collectible in certain research scenarios—such as measuring opinions, attitudes, or perceptions—often necessitates the use of ordinal data, due to the challenges associated with obtaining continuous measurements in these contexts. For instance, eliciting responses on a continuous percentage scale from 0 to 100 can be impractical when assessing subjective experiences or preferences. Hence, the decision to utilize ordinal data over continuous numerical data is not solely a matter of choice in research design and analysis but is also influenced by the practicalities of what data can realistically be gathered. This consideration is crucial for ensuring the validity and relevance of research findings, highlighting the importance of aligning data collection methods with the specific circumstances and constraints of the study.

In summary, while ordinal data is invaluable for capturing rankings and subjective assessments, 
its analytical limitations highlight the superior informational value of continuous numerical data in quantitative research. 
This distinction is crucial for researchers in the selection of appropriate statistical methods and in the interpretation of their data, 
ensuring that conclusions drawn are both valid and meaningful.

\subsection{Clustering}

Clustering is a fundamental technique in the field of data analysis and machine learning, aimed at grouping a set of objects in such a way that objects in the same group (or cluster) are more similar to each other than to those in other groups. Its applications span a wide range of areas including market research, pattern recognition, image analysis, and bioinformatics, among others. For instance, in market research, clustering can help identify distinct customer segments based on purchasing behavior, while in bioinformatics, it can be used to classify different types of genes or proteins with similar functions.

There are primarily two types of clustering: distance-based clustering and model-based clustering.

1. \textbf{Distance-based Clustering}: This approach relies on the concept of similarity or distance between data points. The aim is to minimize the distance between data points within a cluster while maximizing the distance between data points in different clusters. Popular methods include K-means, hierarchical clustering, and DBSCAN, each using different metrics (e.g., Euclidean, Manhattan) to measure distance or similarity.

2. \textbf{Model-based Clustering}: Unlike distance-based methods, model-based clustering assumes that the data is generated from a mixture of finite distributions, where each component of the mixture represents a cluster. This approach tries to estimate the parameters of these distributions to optimize the fit between the model and the data. Model-based methods are particularly useful for complex data structures, including those with mixed types of data (numerical, categorical) or where the clusters have different sizes, shapes, or orientations.

Model-based clustering is often more suitable for ordinal data—data that contain natural, orderable categories. This is because ordinal data, which embody an inherent ranking or order (e.g., survey responses from "strongly agree" to "strongly disagree"), do not fit well with the distance metrics used in distance-based clustering, which treat distances between all pairs of categories as equal. Model-based clustering, on the other hand, can accommodate the ordinal nature by incorporating it into the model assumptions, allowing for a more nuanced understanding and grouping based on the inherent order of the data. This method can capture the ordinal information effectively, providing more meaningful and interpretable clusters for analysis.

\subsection{Prediction}

Prediction in clustering encompasses the task of forecasting where new, unseen data points belong within a pre-established cluster framework, leveraging patterns discerned from a dataset. This process not only builds upon the foundational step of identifying clusters but also utilizes either post-clustering classification algorithms or direct predictive clustering techniques. The core objective is to employ the characteristics of existing clusters—such as their centroids and distribution—to accurately determine the most fitting cluster for each new data point. This approach bridges the gap between mere data grouping and the strategic allocation of future data instances, underscoring the predictive power inherent in a well-structured clustering analysis.

\subsubsection*{key differences between Clustering and Prediction}

\begin{enumerate}
  \item \textbf{Supervision}: 
  Clustering is unsupervised, focusing on pattern discovery without known outcomes. Prediction in clustering, however, may incorporate supervised learning by using clusters as labels for predicting new data point assignments.
  \item \textbf{Objective}:
  The objective of clustering is to find natural groupings within the data. In contrast, prediction in clustering aims to assign new data points to these groups.
  \item \textbf{Approach}:
  While clustering analyzes the dataset to form clusters, predictive modeling in clustering uses these established groups to make future predictions.
  \item \textbf{Data Requirements}:
  Clustering operates on unlabeled data, whereas prediction in clustering may require labeled data derived from the clustering outcome.
  \item \textbf{Outcome}:
  Clustering results in a set of categorized data clusters, whereas prediction in clustering classifies new data points into these clusters.
  
\end{enumerate}

\subsection{Statistical-based Ordinal Data Clustering}

\subsection*{Finite Mixture modeling}

Finite mixture models have emerged as a versatile and powerful statistical tool, garnering increasing attention across various scientific and research disciplines. These models, predicated on the assumption that observed data arise from a blend of several probability distributions, each representing a distinct subpopulation, have revolutionized our approach to understanding complex data structures. The designation ``finite'' in finite mixture models is pivotal, indicating the specific, but variable, number of components or subgroups within the data. This finite aspect offers a balance between model complexity and interpretability, allowing for detailed yet manageable analysis of diverse datasets.

The Finite mixture model is defined as:
\begin{equation}
p(x_i|\mathbf{\Theta}) = \sum_{k=1}^{K} \pi_k f(x_i|\theta_k)
\end{equation}
where:
\begin{itemize}
    \item $p(x_i|\mathbf{\Theta})$ denotes the overall mixture model's density or mass function for observation $x_i$.
    \item $K$ is the total number of component distributions in the mixture.
    \item $\pi_k$ represents the mixing proportion of the $k$th component, satisfying $0 \leq \pi_k \leq 1$ and $\sum_{k=1}^{K} \pi_k = 1$.
    \item $f(x_i|\theta_k)$ is the probability density function (pdf) or probability mass function (pmf) of the $k$th component distribution evaluated at $x_i$.
    \item $\theta_k$ denotes the parameter vector of the $k$th component distribution.
    \item $\mathbf{\Theta}$ symbolizes the complete set of parameters for the mixture model, including both the mixing proportions $\{\pi_1, \ldots, \pi_K\}$ and the parameters of the component distributions $\{\theta_1, \ldots, \theta_K\}$.
\end{itemize}

The significance of finite mixture models is vividly illustrated in Figure~\ref{fig:trend}, which showcases a robust upward trajectory in related publications. This trend not only reflects the growing academic and practical interest in these models but also underscores their evolving sophistication and broadening applicability. From the early advancements in maximizing likelihood estimation through algorithms like EM, as pioneered by McLachlan in 2000, to the more recent developments in handling a variety of data types, including binary, count, and ordinal data, finite mixture models have continuously adapted and expanded their scope.

\begin{figure}[ht!] % 'h!' places the figure here, in the text
    \centering % Centers the figure
    \includegraphics[width=0.6\textwidth]{images/trend.png} % Include the image with 50% of the text width
    \caption{The increase in publications indexed by PubMed that mention a keyword specific to cluster analyses relative to the number of publications 
    that mention a traditional statistical test. 
    Particularly sharp increases can be seen for finite mixture modelling.
    From~\cite{dalmaijer2022statistical}.} % Caption for the image
    \label{fig:trend} % Label for referencing the figure in the text
  \end{figure}


\section{History of Statistical based Ordinal Data Clustering}

\subsection*{Early Developments (2000--2010)}

EM algorithm has been introduced by Dempster, Laird and Rubin in 1977. (~\cite*[Dempster, Laird and Rubin]{Dempster1977}).

McLachlan \& Basford 1988 (~\cite{mclachlan1988mixture}) start to marked a crucial step in mixture model applications by simplifying the maximum likelihood estimation (MLE) using the EM algorithm.

McLachlan \& Peel's 2000 book further elaborate on this approach. This approach, utilizing $Y_j$ and $Z_j$, not only enhanced the computational efficiency of MLE but also laid a foundational strategy for Bayesian approaches and MCMC methods in mixture models. The paper’s impact is evident in its widespread adoption across various domains, from bioinformatics to finance, where mixture models are employed (~\cite{mclachlan2000finite}).

In 2002, Figueiredo's introduction of an unsupervised algorithm for learning finite mixture models was a game-changer. This method's ability to autonomously select the number of components represented a significant leap over previous techniques, which often relied on arbitrary or manual component selection. Additionally, the algorithm's robustness against initialization issues and singular estimates made it a go-to choice for practitioners dealing with complex multivariate data (~\cite{figueiredo2002unsupervised}).

A key paper in 2010 by Volodymyr and Ranjan addressed practical challenges in applying the EM algorithm for mixture models. This comprehensive guide to estimation, model selection, and likelihood maximization was a boon for both researchers and practitioners. Notably, the work extended beyond Gaussian mixtures, offering insights and methodologies for simulating and visualizing non-Gaussian mixtures, thereby broadening the applicability of mixture models (~\cite{10.1214/09-SS053}).

\subsection*{Recent Developments (2010--2019)}

In 2016, Matechou et al. introduced a novel approach to data analysis with their proposal of finite mixture models for biclustering two-mode ordinal categorical data. Utilizing proportional odds parameterization, these models offered a sophisticated understanding of complex data patterns, particularly beneficial in fields such as genomics and social sciences where ordinal data is prevalent. The application of the EM algorithm for model fitting highlighted the continued importance of this method in the context of mixture model applications (~\cite{matechou2016biclustering}).

Similarly, in 2016, Fernandez et al. presented an alternative methodology for clustering ordinal data by employing likelihood-based methods through finite mixtures with the stereotype model. This approach stood out for its use in fuzzy clustering techniques, an area that has been attracting increased attention in the realm of data science (~\cite{fernandez2016mixture}).

In 2019 Fernandez et al.'s extension of finite mixture models to binary, count, and ordinal data under a unified statistical framework represented a consolidation and expansion of mixture model applications. The introduction of maximum likelihood estimation parameters and the Bayesian approach for simultaneous estimation were indicative of the field's progression towards more flexible and comprehensive modeling techniques (~\cite{fernandez2019finite}).

The primary innovation in both of these studies lies in the integration of ordinal data models with finite mixture methods, a departure from the traditional use of finite mixtures for continuous variables.


Jacques and Biernacki's 2018 introduction of a model-based co-clustering algorithm was a significant advancement. The algorithm's ability to handle missing data and its interpretability made it especially relevant for high-dimensional datasets. The BOS distribution employed in this model underscored the continuous innovation in probabilistic modeling techniques, catering to the increasing complexity of data in modern research (~\cite{jacques2018model}).

Compare with previous work, Jacques and Biernacki's (~\cite{jacques2018model}) method is based on a novel Binary Ordinal Search (BOS) model emphasizing efficiency and interpretability, especially with missing data. 

\subsection*{Compare with other clustering algorithm}

Compared to tree based or distance based clustering algorithms, statistical based clustering algorithms demonstrate superior performance with ordinal data, particularly in scenarios involving multiclass and multioutput cases. 
This advantage stems from two key factors: firstly, ordinal data do not presuppose equal distances between categories, a condition that distance-based methods often rely on. Secondly, ordinal data typically encompass only a few categories, which can limit the effectiveness of tree-based methods designed to partition data across a broader numerical range.


\section{Methodology}

The Proportional Odds Model (POM) and the Ordered Stereotype Regression Model (OSRM) are two pivotal approaches in statistical modeling that facilitate the implementation of Factorial Mixture Models (FMM) for ordinal data. Ordinal data is characterized by outcomes that are categories with a natural order, but the differences between these categories are not quantifiable. These methodologies are instrumental in analyzing and interpreting such data, providing a structured framework to understand the relationship between an ordinal response variable and a set of predictor variables.

\subsection{Proportional Odds model}

The Proportional Odds Model is specifically designed for ordinal response variables with ordered categories. It employs a cumulative log odds mechanism to express the probability of the response variable falling into a certain category or below, based on a set of predictors. This model is characterized by its proportional odds assumption, which stipulates a constant effect of predictors across all category thresholds, thereby simplifying the interpretation of the effects of predictor variables on the ordinal outcome.

The Proportional Odds model defined for an ordinal response variable $Y$ with $J$ ordered categories ($j=1, 2, \ldots, J$). 
Given a set of predictor variables represented by the vector $X$, the model expresses the cumulative log odds of $Y$ being less than or equal to a category $j$ as follows:

\[
\log\left(\frac{P(Y \leq j | X = x)}{P(Y > j | X = x)}\right) = \theta_j - \beta^T x
\]

where:
\begin{itemize}
    \item $P(Y \leq j | X = x)$ is the cumulative probability of $Y$ being in category $j$ or any category below $j$, given the predictors $x$.
    \item $\theta_j$ is the intercept term for category $j$, which allows the log odds to vary across categories.
    \item $\beta^T x$ represents the linear predictor, with $X$ being the matrix of predictor variables and $\beta$ the vector of coefficients corresponding to these variables.
    \item The model assumes that the effect of the predictors on the log odds of being in a lower versus a higher category is proportional across all categories, which is encapsulated in the term $\beta^T x$.
\end{itemize}

The Proportional Odds assumption implies that the vector of coefficients $\beta$ is the same across all category thresholds. (~\cite{mccullagh1980regression})

\subsection{Ordered Stereotype Regression Model}

On the other hand, the Ordered Stereotype Regression Model offers a more flexible approach by allowing the effects of predictors to vary across categories. This variation is systematically controlled through a scaling parameter, ensuring that the model adheres to the inherent order within the ordinal data. The Ordered Stereotype Regression Model is particularly useful when the assumption of proportional odds is too restrictive or not met in the data.

The Ordered Stereotype Regression Model is defined where the outcomes are categories with a natural order but not a quantifiable difference between them.

Given an ordinal response variable $Y$ with categories ($j=1, 2, \ldots, J$), and a vector of independent variables $X$, the probability of $Y$ falling into the $j$th category, given $X = x$, is denoted as $P(Y = j | X = x)$.


The logit for category $j$ is then:
\[
\log\left(\frac{P(Y = j | X = x)}{1 - P(Y = 1 | X = x)}\right) = \theta_j + \phi_j \beta^T x
\]
where $\theta_j$ is the intercept for category $j$, and $\beta$ is a vector of coefficients for $X$.
And come with hypothesis
$1 = \phi_1 \geq \phi_2 \geq \ldots \geq \phi_k = 0.$
Stereotype model is unconstrained, $\phi$ add restricts to make it works for ordinal data.
This allows the effects to vary across categories but constrains them to follow a scaled version of a common pattern. (~\cite{anderson1984regression})



\subsection{Expectation-Maximization (EM) algorithm}

The EM (Expectation-Maximization) algorithm is a paramount tool in the realm of statistical analysis, especially in the context of finite mixture modeling for clustering ordinal data. This iterative process, consisting of Expectation (E) and Maximization (M) steps, adeptly handles incomplete data scenarios, offering a methodical approach to estimate model parameters.

The E-Step serves as the foundation for expectation calculation, where latent variables are considered to encapsulate the unknown cluster membership for each data point, leverages the ordered stereotype model or Proportional Odds model to refine the estimation of posterior probabilities.. (~\cite{fernandez2016mixture}) highlight this step's role in assessing the a priori probabilities of data points belonging to various clusters, thereby computing their expected values. This involves evaluating the posterior probabilities that each row (or data point) belongs to a specific row group or cluster, based on current parameter estimates. The E-step effectively sets the stage for the M-step by calculating the conditional expectation of the complete data log-likelihood, incorporating both the a priori probabilities and the observed data to refine the parameter estimates.

The M-Step then capitalizes on the groundwork laid by the E-step, aiming to globally maximize the log-likelihood function derived from the E-step's output. 
This phase is pivotal in updating the estimation of row-cluster (col-cluster) proportions and other model parameters, ensuring the model iteratively converges to the maximum likelihood estimates. 
The M-step's essence lies in its ability to refine parameter estimates, thereby enhancing the model's accuracy in clustering the data points into the correct groups based on the updated posterior probabilities.
the ordered stereotype model's or Proportional Odds model's incorporation is instrumental in optimizing the parameter estimates that define cluster characteristics. 

In the application of the EM algorithm to ordinal data clustering within finite mixture models, the algorithm's iterative nature—alternating between estimating the expected latent variables (E-step) and maximizing the likelihood function with updated parameters (M-step)—is instrumental. This iterative refinement continues until the parameter estimates stabilize, indicating convergence to a solution that best fits the data.

This methodology, as applied to ordinal data clustering, underscores the EM algorithm's versatility and efficacy in uncovering the latent structure within complex datasets. By leveraging the EM algorithm's strengths, researchers can navigate the challenges posed by incomplete data, ultimately enhancing our understanding of the underlying patterns and relationships within ordinal data.

\subsection{Cluster Prediction for Ordinal Data}

Finite Mixture Models (FMMs) offer a robust approach for clustering ordinal data, where the observations are categorized into a natural order but the distances between categories are not necessarily uniform or known. 
This section introduces a streamlined approach for deducing the cluster membership of new observations by utilizing models that have been previously trained, thus bypassing the need for the computationally intensive Expectation-Maximization (E-M) algorithm with each new data instance. 
The discussion will cover the methodology, criteria for deciding the optimal number of clusters, and the wide applicability of this technique. A principal advantage of employing pre-trained FMMs lies in their cost-efficiency. By avoiding the necessity to execute the E-M algorithm repeatedly for fresh datasets, the proposed method significantly reduces expenses, offering a dual benefit of decreased computational demand and enhanced cost savings. This dual benefit renders the approach particularly appealing across various domains, where it provides a scalable and economically viable solution for dynamic data clustering.

\subsubsection*{Approach}

The prediction of new observations' cluster memberships utilizing pre-trained FMMs involves a two-step process. Firstly, a Finite Mixture Model is trained on a dataset where the number of clusters (components) is determined based on model selection criteria such as Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC). This model training step involves the use of the Expectation-Maximization (E-M) algorithm to estimate the parameters of the mixture model, which can be computationally expensive, especially for large datasets.

Once the model is trained, the parameters of each component (cluster) are fixed, and these parameters are used to calculate the likelihood of new observations belonging to each cluster. The key benefit of this approach is that it eliminates the need to re-run the E-M algorithm for each new observation, thereby significantly reducing computational costs and making the prediction process more efficient. This method leverages the probability distributions defined by the pre-trained model to assign new observations to the cluster with the highest likelihood, without updating the model parameters.

\subsubsection*{How to Select the Number of Clusters}

Selecting the appropriate number of clusters for FMMs is crucial for accurate and meaningful clustering. This selection can be guided by several criteria:

\begin{itemize}
    \item \textbf{Information Criteria:} The Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) are popular measures for model selection. They balance model fit and complexity, penalizing models with more parameters. A lower value of BIC or AIC indicates a better model.
    \item \textbf{Cross-Validation:} This involves partitioning the data into subsets, training the model on one subset, and validating it on another to evaluate its predictive performance. The number of clusters that yields the best predictive accuracy on the validation set is chosen.
    \item \textbf{Interpretability and Domain Knowledge:} The selected number of clusters should also make sense in the context of the problem domain. In some cases, domain knowledge can guide the choice, favoring solutions that are interpretable and actionable.
\end{itemize}

\subsubsection*{Applications}

The method of predicting cluster membership for new observations using pre-trained FMMs has broad applications across various fields:

\begin{itemize}
    \item \textbf{Marketing:} Segmenting customers based on purchasing behavior, preferences, or demographic characteristics to tailor marketing strategies.
    \item \textbf{Healthcare:} Classifying patient profiles based on clinical or genetic data to inform treatment plans or identify risk groups.
    \item \textbf{Finance:} Identifying clusters of financial transactions or customer profiles for fraud detection, risk assessment, or personalized financial advice.
    \item \textbf{Social Science:} Analyzing survey data to uncover patterns in attitudes, preferences, or behaviors among different population segments.
\end{itemize}

This approach is particularly beneficial in scenarios where data continuously arrives, and there is a need for quick and cost-effective clustering without sacrificing accuracy. By leveraging pre-trained models, organizations can dynamically assign new data to clusters, enabling real-time insights and decision-making.

\section{Research Goals}

This research aims to systematically analyze and compare the performance of different predictive models in clustering ordinal data derived from various distributions. Recognizing that the clustering of ordinal data under normal distribution conditions has been well-researched, this work shifts the focus towards less commonly explored distributions, such as Bernoulli, Poisson, and others, to understand their impact on model performance. The primary objective is to elucidate how these types of distributions affect the models' ability to accurately predict the cluster memberships of ordinal data. By doing so, we intend to:

\begin{enumerate}
    \item \textbf{Evaluate Model Robustness:} Assess the adaptability and robustness of each predictive model in accurately clustering ordinal data from diverse distributions, with a particular focus on non-normal distributions like Bernoulli and Poisson. This includes a comprehensive examination of performance metrics under various distributional conditions to identify each model's strengths and limitations.
    \item \textbf{Identify Optimal Clustering Models:} Determine which models consistently demonstrate superior performance in predicting clusters for ordinal data across a broad spectrum of non-normal distributions. This will aid in recommending specific models for practitioners tasked with clustering ordinal data, guiding them to the most effective tools based on the data's distribution characteristics.
    \item \textbf{Improve Clustering Accuracy:} Through detailed comparative analysis, this study aims to offer insights that could lead to improved accuracy in clustering ordinal data. Understanding the impact of different distributions, especially non-normal ones, on model performance will allow for the customization of clustering approaches to better match the data's properties, thus enhancing prediction reliability.
    \item \textbf{Advance Methodological Approaches:} The findings from this research are expected to make significant contributions to the methodological advancements in the field of data analytics, especially in clustering ordinal data. Highlighting the relationship between data distributions, particularly non-normal distributions, and model performance will direct future research toward the development of more adaptive and sensitive clustering techniques.
\end{enumerate}

This research will provide a thorough empirical analysis on the critical aspect of distributional sensitivity in predictive modeling for clustering ordinal data, facilitating more informed model selection and leading to more accurate cluster prediction in various application domains.

\printbibliography

\end{document}
